{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms.html\n",
    "\n",
    "LLMs are used at multiple different stages of your pipeline:\n",
    "\n",
    "- During Indexing you may use an LLM to determine the relevance of data (whether to index it at all) or you may use an LLM to summarize the raw data and index the summaries instead.\n",
    "\n",
    "- During Querying LLMs can be used in two ways:\n",
    "  - During Retrieval (fetching data from your index) LLMs can be given an array of options (such as multiple different indices) and make decisions about where best to find the information youâ€™re looking for. An agentic LLM can also use tools at this stage to query different data sources.\n",
    "  - During Response Synthesis (turning the retrieved data into an answer) an LLM can combine answers to multiple sub-queries into a single coherent answer, or it can transform data, such as from unstructured text to JSON or another programmatic output format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you will instantiate an LLM and pass it to Settings, which you then pass to other stages of the pipeline, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RicardoMontaner\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Set the LLM, tokenizer and embed model\n",
    "import os\n",
    "\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.replicate import Replicate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_EGiIWBdx31PpO5ApNyuEuiW2t8jMueV2LEG1L\"\n",
    "\n",
    "# from llama_index.llms.ollama import Ollama\n",
    "# https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoLlama-Local.html\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html\n",
    "# Settings.llm = Ollama(model=\"llama2\", request_timeout=60.0, temperature=0.5)\n",
    "# set the LLM\n",
    "llama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\n",
    "Settings.llm = Replicate(\n",
    "  model=llama2_7b_chat,\n",
    "  temperature=0.01,\n",
    "  additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "\n",
    "# set tokenizer to match LLM\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "# set the embed model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../docs\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Readers from LlamaHub\n",
    "Because there are so many possible places to get data, they are not all built-in. Instead, you download them from our registry of data connectors, LlamaHub.\n",
    "\n",
    "In this example LlamaIndex downloads and installs the connector called DatabaseReader, which runs a query against a SQL database and returns every row of the results as a Document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import download_loader\n",
    "\n",
    "# from llama_index.readers.database import DatabaseReader\n",
    "\n",
    "# reader = DatabaseReader(\n",
    "#     scheme=os.getenv(\"DB_SCHEME\"),\n",
    "#     host=os.getenv(\"DB_HOST\"),\n",
    "#     port=os.getenv(\"DB_PORT\"),\n",
    "#     user=os.getenv(\"DB_USER\"),\n",
    "#     password=os.getenv(\"DB_PASS\"),\n",
    "#     dbname=os.getenv(\"DB_NAME\"),\n",
    "# )\n",
    "\n",
    "# query = \"SELECT * FROM users\"\n",
    "# documents = reader.load_data(query=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Documents directly\n",
    "# Instead of using a loader, you can also use a Document directly.\n",
    "\n",
    "# from llama_index.core import Document\n",
    "\n",
    "# doc = Document(text=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x28383dd7650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, this splits your Document into Node objects, which are similar to Documents (they contain text and metadata) but have a relationship to their parent Document.\n",
    "\n",
    "If you want to customize core components, like the text splitter, through this abstraction you can pass in a custom transformations list or apply to the global Settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "## Separa los documentos en chunks de 512 caracteres con un overlap de 10\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[text_splitter]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save vectorStore\n",
    "index.storage_context.persist(persist_dir=\"./vectorStore\")\n",
    "# graph.root_index.storage_context.persist(persist_dir=\"./vectorStore\")\n",
    "\n",
    "## Load vectorStore\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./vectorStore\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)\n",
    "# Important: if you had initialized your index with a custom transformations, embed_model, etc., \n",
    "# you will need to pass in the same options during load_index_from_storage, or have it set as the global settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeChromaDB():    \n",
    "  import chromadb\n",
    "  from llama_index.core import StorageContext\n",
    "\n",
    "  # initialize client, setting path to save data\n",
    "  db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "  # create collection\n",
    "  chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "  \n",
    "  return chroma_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# create your index\n",
    "chroma_collection = initializeChromaDB()\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context\n",
    "# )\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As a responsible and ethical AI language model, I must inform you that I cannot provide a definitive answer to your question about the meaning of life. The concept of \"meaning\" is complex and subjective, and there are countless philosophical, religious, scientific, and cultural perspectives on what constitutes the meaning of life.\n",
      "However, I can offer some insights based on various philosophical and psychological theories. Some possible answers to this question could include:\n",
      "* Biological perspective: Life has evolved over millions of years through natural selection, and its primary purpose is to perpetuate the survival and reproduction of organisms. From this viewpoint, the meaning of life is simply to continue existing and passing on one's genetic material to future generations.\n",
      "* Psychological perspective: According to Maslow's hierarchy of needs, the meaning of life is found in fulfilling our basic physiological and safety needs, followed by social and esteem needs, and finally self-actualization needs. In other words, life has meaning when we feel secure, loved, respected, and fulfilled.\n",
      "* Religious perspective: Many religious traditions teach that the meaning of life is to serve a higher power or follow divine commandments. For example, some believe that life's purpose is to love God and follow His will, while others believe it is to achieve spiritual enlightenment or liberation from the\n"
     ]
    }
   ],
   "source": [
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the meaning of life?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
